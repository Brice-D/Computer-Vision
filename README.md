# Image Classification


## Projektbeschreibung

Bei diesem Projekt geht es darum mit dem Datensatz "Fruits-360" (bei Kaggle zu finden) den Einfluss von Hyperparametern auf ein Model darzustellen.
DafÃ¼r wurde als LÃ¶sungsansatz "Transfer Learning" ausgewÃ¤hlt. Es wird spÃ¤ter mehr darauf eingegangen. 

## Fruits-360

<p align="center">
	<img src="images/6_100.jpg">
</p>

[fruits-360](https://www.kaggle.com/moltean/fruits)

Dieser Datensatz besteht aus 90483 Obst-Bilder, die in 131 Klassen (Avocado, Apfel, Kirschen etc.) unterteilt sind. Diese ganze Bilder wÃ¼rden in 3 Verzeichnisse verteilt, das erste enthÃ¤lt Bilder fÃ¼r das Training (67692 Bilder), das nÃ¤chste enthÃ¤lt Bilder fÃ¼r das Testen (22688) und das letzte 103 Bilder auf denen mehreren Obst zu sehen sind. Das letztere Verzeichnis wird jedoch wÃ¤hrend dieser Arbeit nicht verwendet. Die Bilder haben eine GrÃ¶ÃŸe von 100x100 Pixel und der Datensatz wurde fÃ¼r das letzte Mal am 18. Mai 2020 aktualisiert. 

## Ã„hnliche Arbeiten 

Es gibt auf Kaggle eine Menge Arbeiten, die auf diesem Datensatz basiert und es wurde schon eine Genauigkeit von 98% erreicht. Aus diesem Grund ist der Fokus dieser Arbeit nicht eine bessere Accuracy zu erreichen sondern Model zu trainieren mit unterschiedlichen Parametern und zu beobachten wie die ausgewÃ¤hlte Parameter auf das Ergebnis einwirken und falls ungewollte Fehler auftreten, diese zu korrigieren.

## Transfer Learning 

Bei **Transfer Learning** wird ein schon trainiertes Model fÃ¼r eine spezifische TÃ¤tigkeit verwendet und dieses auf eine Ã¤hnliche TÃ¤tigkeit angewendet.
Diese Optimierung ermÃ¶glicht es bei der neuen TÃ¤tigkeit eine schnelle LÃ¶sung und Ã¶fter eine bessere LÃ¶sung fÃ¼r erreichen.
Diese Methode basiert auf 3 Schritte when man ein schon trainiertes Model weiterverwenden will: 
  - Base Model auswÃ¤hlen: ein schon trainiertes Model soll ausgewhÃ¤hlt werden, am bestens ein Model, das schon mit Ã¤hnlichen Daten wie die neuen Daten trainiert         wurde. 
  - Wiederverwendung des ausgewÃ¤hlten Model: das ausgewÃ¤hlte Model kann dann fÃ¼r die neue TÃ¤tigekeit eingesetzt werden. Bei dem Einsatz kann das ganze Model ohne 
    weitere Ã„nderung oder mit Ã„nderungen verwendet werden.
  - Bei dem letzten Schritt geht es darum, das mÃ¶del zu verbessern, um ein besseres Ergebnis zu erreichen. 

Transfer Learning ist nÃ¼tzlich wenn der Entwickler Zeit sparen mÃ¶chte, nicht genug Daten hat und eine bessere Perfotmanz erzielen mÃ¶chte. 

## ResNet50 

Da Transfer learning als LÃ¶sungsansatz ausgewÃ¤hlt wurde, wird ResNet50 als Base-Model fÃ¼r diese Arbeit eingesetzt.

ResNet50 ist eine CNN bestehend aus 50 Layers. Das Frameworks keras stellt ein schon trainiertes Model von ResNet50 zur VerfÃ¼gung, dieses Model wurde mit dem Datensatz **ImageNet** trainiert. Dieses erzielte eine Top-1 Genauigkeit von 75% und eine Top-5 Genauigkeit von 92%.

## Implementierung

Es wurden insgesamt 5 Modele basierend auf ResNet50 trainiert, die Modele wurden nach und nach verbessert, um bessere Ergebnisse zu erzielen in dem Hyperparameter immer angepasst wurden. Die Plot von allen trainierten Modellen werden hier dargestellt und erklÃ¤rt.
Zu allen Modelen wurden vie neue Layers hinzugefÃ¼gt:
- Zwei Dense Layer, mit "relu" als Aktivierungsfunktion, einmal 512 und 256 Units fÃ¼r die Dimension des Outputs 
- zwischen diesen Dense() Layers sind Dropout() Layers mit einer Rate von 25%
- Der Output Layer ist ebensfalls ein Dense() mit "softmax" als Aktivierungsfunktion und 131 Units, weil die DatensÃ¤tze Ã¼ber 131 Klassen verfÃ¼gt. Softmax ist immer
  Softmax ist die Funktion, die am bestens passt fÃ¼r eine Klassifiezierung von mehr als 2 Klassen.

FÃ¼r die zwei ersten Modelle wurden die DatensÃ¤tze so aufgeteilt:
 - Training-Verzeichnis: 
 	- 80% (54190 Bilder) fÃ¼r das Training
	- 20% (13502 Bilder) fÃ¼r die Validation
- Test-Verzeichnis: 
	- 100% (22688 Bilder) fÃ¼r den Test
 
Die Implementierung erfolgte mit Google Colab:  [QuelleCode](https://github.com/Brice-D/Computer-Vision/blob/master/project.ipynb). 
## Model 1

Beim ersten Training wurde das ResNet50 Model geladen, der letzte Layers deaktiviert "freeze" und die anderen Layers des Models so gesetzt, dass diese nicht trainiert werden. 

Das Ergebnis des ersten Trainings ist im unten stehenden Bild zu sehen. 

<p align="center">
	<img src="images/Test1.jpeg">
</p>

Wie auf dem Bild zu sehen ist, hat das Model nicht wirklich gelernt, weil die Accuracy fÃ¼r die Validation Bilder nicht mal 1% erreicht hat. Das erste Model wird mit den Testbildern evaluiert und ergibt eine Genauigkeit von 0,71%. Von den 22688 Testbildern kÃ¶nnen nur 163 Bilder richtig erkannt werden. (very bad result ğŸ˜¢ )

## Model 2

Hier ging es darum alle Layers von ResNet50 10 Epochen zu trainieren.

<p align="center">
	<img src="images/Test2.jpeg">
</p>

Wie dem Bild zu entnehmen ist, konnte hier ein besseres Ergebnis erreicht werden. Jedoch merkt man das es zwischen den Epochen [2 4] und [6 9] zu eineim mÃ¶glichen Overfitting kommt, weil in diesen Bereichen die Loss grÃ¶ÃŸer wird. 
Beim nÃ¤chsten Ansatz wird versucht das Problem zu lÃ¶sen. 

## Model 3

Das Problem des Overfittings, das taucht beim zweiten Model wollen wir hier lÃ¶sen. Das Problem taucht auf wenn das trainierte Model eine schlechte Genauigkeit ergibt mit neuen DatensÃ¤tze, einfach erklÃ¤rt: training loss << validation loss. 

Es gibt mehrere Methoden das Overfitting zu vermeiden (hier werden nur 2 zwei einfachsten erwÃ¤hnt)
- Das Model mit mehr Daten trainieren
- Early stopping: hier geht es darum das Model mit weningen Epochen zu trainieren. Bis zu einem bestimmten Punkt wird das Model verbessert, Nach diesem Punkt kann     die FÃ¤higkeit des Modells zur Verallgemeinerung jedoch nachlassen, wenn es beginnt, die Trainingsdaten zu Ã¼berlagern. Die Idee ist dann das Model nur bis zu diesem Punkt zu trainieren. 

<p align="center">
	<img src="images/early-stopping-graphic.jpg">
</p>

Die ganze 67692 Bilder im Training-verzeichnis wurden fÃ¼r das Training verwenden und die Bilder im Test-Verzeichnis wurden so aufgeteilt, dass 80% (18207) fÃ¼r das Testen und 20% (4481) fÃ¼r die Validation verwendent wurde. 

Das Ergebnis des Models ist im unten stehenden Bild zu entnehmen.

<p align="center">
	<img src="images/Test3.jpeg">
</p>

Das Problem wurde gelÃ¶st und eine Genauigkeit von fast 99% mit einem loss von 7.906072823971044e-06 wurden erreicht. Von den 18207 Testbilder konnten nur 185 Bilder nicht erkannt werden. (top result ğŸ‘ŒğŸ¾ğŸ‘ŒğŸ¾ ğŸ˜Š )
Der nÃ¤chste Ansatz ist es das Model mit der gleichen Aufteilung des Datensatzes lÃ¤nger zu trainieren, um zu gucken ob eine noch bessere Ergebnis erzielt werden kann.

## Model 4 

Das Model 4 ist das gleiche wie das dritte, das Trainieren dauert aber lÃ¤nger anstatt 10 Epochen zu trainieren, wurde hier 30 Epochen trainiert. 

<p align="center">
	<img src="images/Test4.jpeg">
</p>

Das Ergebnis im Plot zeigt wie die Kurven schlecht sind, dies beweist den Einsatz von "Early stopping", um das Overfitting zu vermeiden.
Das trainig loss und validation loss sind nicht stabil. 

## Model 5

Interessant wÃ¤re auch zu beobachten, wie die learning rate das Ergebnis des Trainings beeinflussen kann. 
Die Lernrate ist ein Hyperparameter, der steuert, wie sehr wir die Gewichte unseres Netzwerks unter BerÃ¼cksichtigung des Verlustgradienten anpassen. Die folgende Formel zeigt die AbhÃ¤ngigkeit: new_weight = existing_weight â€” learning_rate * gradient

<p align="center">
	<img src="images/Test5.jpeg">
</p>

Die Learnrate wurde reduziert von learning_rate=1e-4 auf learning_rate=1e-5. Das Ergebnis ist nicht schlecht aber im Vergleich zu dem dritten Model, konnte hier 231 Bilder nicht erkannt werden. Das sind mehr Bilder als mit dem Model 3, das nur 185 Bilder nicht erkennen konnte.

## Zusammenfassung 

Wie die Graphiken und die Ergebnisse gezeigt haben, spielen die Hyperparameters eine groÃŸe Rolle im Machine Learning Prozess. Diese mÃ¼ssen sorfÃ¤llig ausgewÃ¤hlt werden. FÃ¼r bestimmte Probleme, die sich Ã¤hneln kann man Parameters, die in der Vergangenheit ein gutes Ergebnis geliefert haben einsetzen. Es gibt keine feste Parameters die zu jedem Problem das beste Ergebnis ergibt, diese kÃ¶nnen erst nach mehreren Versuchen gefunden werden.
Neben den Hyperparametern, spliet auch die Aufteilung der Messdaten eine groÃŸe Rolle wie in dieser Arbeit gezeigt wurde. Die Aufteilung in Training, Validation und Test-Set ist bevorzugt, um mit dem Test-Set, nach dem Training das Model mit unbekanntes (unseen) Dataset zu evaluieren. Wichtig ist auch die Menge der Daten, diese sollte mÃ¶glich viel sein und das Trainieren am bestens mit sehr viel Daten bilden. Das Framework Keras stellt Methoden zur Data Augmentation zur VerfÃ¼gung, diese wurden bei der Implementierung eingesetzt.


## Quellen

- [Dataset](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)
- [Transfer learning](https://builtin.com/data-science/transfer-learning)
- [Overfitting](https://elitedatascience.com/overfitting-in-machine-learning)
- [Learning rate](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10)
